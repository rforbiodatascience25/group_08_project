---
title: "01_load_new"
author: "Eleni"
format: html
editor: visual
---

## 0. Load required packages

```{r}

# Install required packages only if missing
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

required <- c("tidyverse", "stringr", "janitor", 
              "AnnotationDbi", "ath1121501.db")

for (pkg in required) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    BiocManager::install(pkg, update = FALSE, ask = FALSE)
  }
}

library(tidyverse)
library(stringr) # for string operations (pattern matching, cleaning text, etc.)
library(janitor) # for cleaning column names and simple data cleaning functions, helps tidy data frames
library(AnnotationDbi) # provides functions to map probe IDs to gene IDs toretrieve annotations and work with Bioconductor genomic databases.
library(ath1121501.db) # Affymetrix Arabidopsis ATH1 annotation database
```

## 1. **Read Original Series Matrix Files (With Metadata)**

```{r}
#These GEO Series Matrix files contain:
#   - an expression matrix (gene probes × samples)
#   - multiple metadata lines (starting with '!', '#', or '^')
# Because of this, we read them line-by-line instead of loading as a table.

file_5620 <- "../_raw/data/GSE5620_series_matrix.txt.gz" # CONTROL
file_5621 <- "../_raw/data/GSE5621_series_matrix.txt.gz" # STRESS

#Read the files as plain text (line by line) so metadata can be parsed
raw_5620 <- readLines(file_5620)
raw_5621 <- readLines(file_5621)
```

## **2. Extract Sample Metadata (Titles + GSM IDs)**

```{r}
# This function extracts sample info (titles + GSM IDs) from the GEO file.
# It now also looks inside each sample title and automatically assigns
# a condition based on whether the title contains "Control" or "Cold".
extract_metadata <- function(raw_lines) {
  
  # Find the line with all sample titles
  title_line <- raw_lines[str_detect(raw_lines, "^!Sample_title")]
  
  # Find the line with all GSM IDs
  gsm_line <- raw_lines[str_detect(raw_lines, "^!Sample_geo_accession")]
  
  # Remove the part before the first tab so we keep only values
  title_clean <- str_remove(title_line, "^!Sample_title\t")
  gsm_clean   <- str_remove(gsm_line, "^!Sample_geo_accession\t")
  
  # Split into individual entries
  titles <- str_split(title_clean, "\t")[[1]]
  gsm    <- str_split(gsm_clean,   "\t")[[1]]
  
  # Remove quotes
  titles <- str_remove_all(titles, '"')
  gsm    <- str_remove_all(gsm, '"')
  
  # Determine condition from the title text
  # We convert titles to lowercase to make matching easier.
  condition <- case_when(
    str_detect(tolower(titles), "control") ~ "control",
    str_detect(tolower(titles), "cold")    ~ "cold",
    TRUE ~ "unknown"   # fallback if neither appears
  )
  
  # Return the metadata table
  tibble(GSM = gsm, title = titles, condition = condition)
}

# Extract metadata from each dataset
meta_5620 <- extract_metadata(raw_5620)
meta_5621 <- extract_metadata(raw_5621)

```

## 3. **Parse Metadata Into Structured Variables**

```{r}
# This function extracts useful pieces of information from the sample title.
# Each title contains the tissue type, the timepoint, and the replicate number.
# We pull these out using pattern matching.
parse_title <- function(df) {
  df |>
    mutate(
      # Extract whether the sample is from Shoots or Roots
      tissue = str_extract(title, "(Shoots|Roots)"),

      # Extract the timepoint (e.g., "0h", "6.0h")
      timepoint = str_extract(title, "[0-9.]+h"),

      # Extract the replicate number (e.g., "Rep1")
      replicate = str_extract(title, "Rep[0-9]")
      
      # No need for a treatment column here,
      # because we already have a clean 'condition' column.
    )
}

# Apply the parsing function to the metadata for each dataset
meta_5620 <- parse_title(meta_5620)
meta_5621 <- parse_title(meta_5621)

# Combine the two metadata tables into one complete table
sample_metadata <- bind_rows(meta_5620, meta_5621)

```

## 4. Save clean metadata table

```{r}
# Create "_processed" inside the current project folder if it doesn't exist (it wont crush if you rerun the code)
if (!dir.exists("_processed")) dir.create("_processed")

# Save the clean metadata inside that folder
write_csv(sample_metadata, "_processed/sample_metadata.csv")

```

## 5. **Load Expression Matrices (numeric table only)**

```{r}
# These two lines read the numeric expression matrices from the original GEO Series Matrix files. By using comment.char = "!", we tell R to skip all lines starting with "!", which are metadata lines (we have already extracted what we need).
# Only the actual expression table (probe IDs + expression values) is loaded.
# row.names = 1 sets the first column (the probe IDs) as row names.

gse5620 <- read.delim(file_5620, comment.char = "!", row.names = 1)
gse5621 <- read.delim(file_5621, comment.char = "!", row.names = 1)

# Combine the two datasets side by side.
# The rows are probe IDs, so cbind() matches them automatically.
# This gives us one full expression matrix with all samples.
expr_all <- cbind(gse5620, gse5621)

```

## **6. Annotate Probe IDs → Gene Symbols**

```{r}
# 1. Get annotation
probe_to_gene <- AnnotationDbi::select(
  ath1121501.db,
  keys = rownames(expr_all),
  columns = c("PROBEID", "GENENAME", "SYMBOL"),
  keytype = "PROBEID"
)

# 2. Clean column names → probeid, genename, symbol
probe_to_gene <- janitor::clean_names(probe_to_gene)

# 3. Remove duplicated probe IDs (rare but safe)
probe_to_gene <- dplyr::distinct(probe_to_gene, probeid, .keep_all = TRUE)

# 4. Prepare expression matrix with probeid column
expr_all2 <- expr_all |>
  as.data.frame() |>
  tibble::rownames_to_column("probeid")    # <-- uses SAME NAME as annotation

# 5. Join directly (no rename needed)
expr_annotated <- dplyr::left_join(
  probe_to_gene,
  expr_all2,
  by = "probeid"
)


```

## 7. **Save Annotated Expression Matrix**

```{r}
# Save the fully annotated expression matrix before filtering.
# This file includes:
#   - probe IDs
#   - gene symbols
#   - gene names
#   - expression values for all 60 samples
#
# This serves as a complete reference version of the dataset.
# The filtered version (created in Step 8) will be used for downstream analysis (PCA, differential expression, clustering, heatmaps, etc.).

write_csv(expr_annotated, "_processed/expression_annotated.csv")
```

## 8. Filter Uninformative Genes

```{r}
# a. Remove Affymetrix control probes (start with "AFFX") ---------------
# These are technical control probes and do not represent real genes.
expr_filtered <- expr_annotated |>
  filter(!grepl("^AFFX", probeid))

# b. Remove probes without a valid gene symbol 
# Probes with no gene symbol cannot be interpreted biologically.
expr_filtered <- expr_filtered |>
  filter(!is.na(symbol) & symbol != "")


```

```{r}
# Identify expression columns (all GSM samples)
gsm_cols <- grepl("^GSM", names(expr_filtered))

# Log2 transform IN PLACE: expr_filtered is now log2-normalised
expr_filtered[ , gsm_cols] <- log2(expr_filtered[ , gsm_cols] + 1)

# Numeric matrix of expression only
expr_numeric <- expr_filtered[ , gsm_cols]

```

```{r}
# c. Remove low-variance genes (bottom 20%)
# Genes with very little variation across samples are uninformative and add noise.
gene_variance <- apply(expr_numeric, 1, var)

# Cutoff at 20th percentile
variance_cutoff <- quantile(gene_variance, 0.20)

# Keep only genes with variance above cutoff
expr_filtered <- expr_filtered[gene_variance > variance_cutoff, ]
```

## 9. Save the final filtered expression matrix

```{r}
# This dataset contains only informative genes:
#   - no control probes (AFFX)
#   - no unannotated probes
#   - only genes with meaningful variation across samples
#   -log2 normalized values
# This is the dataset that should be used for all downstream analyses:


write_csv(expr_filtered, "_processed/expression_filtered_normalized.csv")

```

## 10. Merge normalized expression with metadata

```{r}
# 1. Load expression + metadata
# expr_filtered = final filtered & normalized expression matrix
# sample_metadata = GSM-level metadata (condition, tissue, timepoint, etc.)

expr <- expr_filtered
meta <- sample_metadata

# 2. Keep only GSM expression columns
# We remove annotation columns because PCA uses only numeric expression.
# Then we assign gene symbols as rownames 

# Keep only columns that contain expression values
expr_only <- expr |> 
  dplyr::select(starts_with("GSM"))

# Gene symbols often appear multiple times (duplicate probes)
# -> make.unique() ensures safe, valid, unique names 
safe_symbols <- make.unique(expr$symbol)

# Assign unique gene symbols as row names of the expression matrix
rownames(expr_only) <- safe_symbols

# 3. Transpose the matrix
# PCA requires: samples as rows, genes as columns.
# Expression matrices from GEO normally have the opposite format.

expr_t <- t(expr_only)        # transpose -> GSM samples become rows
expr_t <- as.data.frame(expr_t)

# Add GSM ID column from rownames so we can merge metadata later
expr_t$GSM <- rownames(expr_t)

# 4. Merge metadata into the expression table
# Each row is now a GSM sample → we join metadata by GSM ID.

expr_with_meta <- dplyr::left_join(expr_t, meta, by = "GSM")

# 5. Reorder columns
# For plotting/PCA it's much easier if metadata columns come first, followed by all gene expression columns.

expr_with_meta <- expr_with_meta |>
  dplyr::select(
    GSM,            # sample ID
    title,          # full sample title
    condition,      # control vs stress
    tissue,         # Shoots / Roots
    timepoint,      # 0h, 0.25h, 1h, ...
    replicate,      # Rep1 / Rep2
    dplyr::everything()   # then all gene expression columns
  )

# 6. Save final dataset
# This file includes: GSM + metadata + log2-normalized gene expression
write_csv(expr_with_meta, "_processed/expression_with_metadata.csv")

```
